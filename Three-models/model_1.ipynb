{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = pd.read_csv(\"./data/x_train.csv\", header=None)\n",
    "df_y = pd.read_csv(\"./data/y_train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_X = df_X.to_numpy()\n",
    "np_y = df_y.to_numpy()[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-17.556392250000002)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_y = np.median(np_y)\n",
    "median_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate - доля точек, которые мы отнесем к первой группе \n",
    "# В данном случае rate=0.5, т.е. точек в двух группах поровну\n",
    "rate = 0.5\n",
    "# Производим разбиение точек по степени близости y к median_y\n",
    "# Для нахождения необходимого значения используем бинарный поиск по ответу\n",
    "L_d, R_d = 0, np.max(np_y) - np.min(np_y)\n",
    "for i in range(200):\n",
    "    M_d = (L_d + R_d) / 2\n",
    "    np_X_0, np_X_1 = [], []\n",
    "    np_y_0, np_y_1 = [], []\n",
    "    for i in range(np_X.shape[0]):\n",
    "        if median_y - M_d <= np_y[i] <= median_y + M_d:\n",
    "            np_X_0.append(np_X[i])\n",
    "            np_y_0.append(np_y[i])\n",
    "        else:\n",
    "            np_X_1.append(np_X[i])\n",
    "            np_y_1.append(np_y[i])\n",
    "    if len(np_y_0) / len(np_y) < rate:\n",
    "        L_d = M_d\n",
    "    else:    \n",
    "        R_d = M_d\n",
    "d_y = M_d\n",
    "\n",
    "np_X_0, np_X_1 = [], []\n",
    "np_y_0, np_y_1 = [], []\n",
    "# Значения массива np_y_bin отражают, к какой из групп относится точка\n",
    "# 0 - первая группа\n",
    "# 1 - вторая группа\n",
    "# Этот массив необходим для обучения и проверки нейросети\n",
    "np_y_bin = []\n",
    "for i in range(np_X.shape[0]):\n",
    "    if median_y - M_d <= np_y[i] <= median_y + M_d:\n",
    "        np_X_0.append(np_X[i])\n",
    "        np_y_0.append(np_y[i])\n",
    "        np_y_bin.append(0)\n",
    "    else:\n",
    "        np_X_1.append(np_X[i])\n",
    "        np_y_1.append(np_y[i])\n",
    "        np_y_bin.append(1)\n",
    "np_X_0 = np.array(np_X_0)\n",
    "np_y_0 = np.array(np_y_0)\n",
    "np_X_1 = np.array(np_X_1)\n",
    "np_y_1 = np.array(np_y_1)\n",
    "np_y_bin = np.array(np_y_bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median_y: -18, d_y: 72, rate = 0.5\n",
      "Len_0 = 150, Len_1 = 150\n"
     ]
    }
   ],
   "source": [
    "# Выводим данные о классификации\n",
    "print(f\"median_y: {round(median_y)}, d_y: {round(d_y)}, rate = {rate}\")\n",
    "print(f\"Len_0 = {len(np_y_0)}, Len_1 = {len(np_y_1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим нейронную сеть для задачи регрессии точек группы 1 (не близких к медианном значению)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.X[i]).float(), torch.tensor(self.y[i]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNRegression:\n",
    "    def __init__(self, n_hidden=128, n_hidden_layers=2, lr=1e-3, n_epochs=200, batch_size=32, sigmoid_rate=0):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        # sigmoid_rate - доля sigmoid среди функций активации\n",
    "        self.sigmoid_rate = sigmoid_rate\n",
    "        self.model = None\n",
    "        self.train_loss = None\n",
    "        self.val_loss = None\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        n_val_X = X_train.shape[-1]\n",
    "        n_val_y = y_train.shape[-1]\n",
    "\n",
    "        lawyers = [nn.Linear(n_val_X, self.n_hidden), nn.ReLU()]\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            lawyers.append(nn.Linear(self.n_hidden, self.n_hidden))\n",
    "            if random.random() < self.sigmoid_rate:\n",
    "                lawyers.append(nn.Sigmoid())\n",
    "            else:\n",
    "                lawyers.append(nn.ReLU())\n",
    "        lawyers.append(nn.Linear(self.n_hidden, n_val_y))\n",
    "        self.model = nn.Sequential(*lawyers)\n",
    "        \n",
    "        optim = Adam(self.model.parameters(), lr=self.lr)\n",
    "        dataset = RegressionDataset(X_train, y_train)\n",
    "        dataloader = DataLoader(dataset, shuffle=True, batch_size=self.batch_size)\n",
    "        self.train_loss = []\n",
    "\n",
    "        val_dataset = RegressionDataset(X_val, y_val)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "        self.val_loss = []\n",
    "\n",
    "        best_model = None\n",
    "        best_num_epoch = None\n",
    "        best_val_loss = None\n",
    "        \n",
    "        for epoch in range(1, self.n_epochs + 1):\n",
    "            losses = []\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                y_pred = self.model(x_batch)\n",
    "                loss = F.mse_loss(y_pred, y_batch)\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                losses.append(loss.detach().item())\n",
    "            self.train_loss.append(np.mean(losses))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                losses = []\n",
    "                for x_batch, y_batch in val_dataloader:\n",
    "                    y_pred = self.model(x_batch)\n",
    "                    loss = F.mse_loss(y_pred, y_batch)\n",
    "                    losses.append(loss.detach().item())\n",
    "                self.val_loss.append(np.mean(losses))\n",
    "            \n",
    "            if best_val_loss == None or best_val_loss > self.val_loss[-1]:\n",
    "                best_val_loss = self.val_loss[-1]\n",
    "                best_num_epoch = epoch\n",
    "                best_model = deepcopy(self.model)\n",
    "            if epoch % 100 == 0:\n",
    "                # Вывод сведений о процессе обучения\n",
    "                print(f\"Epoch {epoch}, loss {round(self.val_loss[-1])}\")\n",
    "        self.model = deepcopy(best_model)\n",
    "        # Лучшая модель\n",
    "        print(f\"Best loss was at {best_num_epoch} epoch: {round(best_val_loss)}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor(X).float()\n",
    "            y = self.model(X).numpy()\n",
    "            return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Гиперпараметры\n",
    "n_hidden_1 = 3\n",
    "n_hidden_layers_1 = 5\n",
    "lr_1 = 1e-3\n",
    "n_epochs_1 = 6000\n",
    "batch_size_1 = 4\n",
    "sigmoid_rate_1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_val_1, y_train_1, y_val_1 = train_test_split(np_X_1, np_y_1, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убираем лишнюю размерность\n",
    "y_train_1 = y_train_1.reshape(-1, 1)\n",
    "y_val_1 = y_val_1.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = NNRegression(n_hidden=n_hidden_1, n_hidden_layers=n_hidden_layers_1, lr=lr_1, n_epochs=n_epochs_1, batch_size=batch_size_1, sigmoid_rate=sigmoid_rate_1)\n",
    "model_1.fit(X_train_1, y_train_1, X_val_1, y_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График функции потерь на тренировочной и валидационной выборках\n",
    "\n",
    "# Начинаем строить график, начиная с конкретной эпохи\n",
    "cut_start_1 = 0.1\n",
    "# Начинаем строить график, заканчивая конкретной эпохой\n",
    "cut_end_1 = 0\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(np.arange(int(len(model_1.train_loss) * cut_start_1), len(model_1.train_loss) - int(len(model_1.train_loss) * cut_end_1)), \n",
    "         model_1.train_loss[int(len(model_1.train_loss) * cut_start_1):len(model_1.train_loss) - int(len(model_1.train_loss) * cut_end_1)])\n",
    "plt.plot(np.arange(int(len(model_1.val_loss) * cut_start_1), len(model_1.val_loss) - int(len(model_1.val_loss) * cut_end_1)), \n",
    "         model_1.val_loss[int(len(model_1.val_loss) * cut_start_1):len(model_1.val_loss) - int(len(model_1.val_loss) * cut_end_1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE: {round(mean_squared_error(y_val_1, model_1.predict(X_val_1)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = 0\n",
    "for file_name in os.listdir(\"./models_1\"):\n",
    "    if file_name.startswith(\"model_1\") and file_name.endswith('.pth'):\n",
    "        cur += 1\n",
    "# Сохраняем модель\n",
    "torch.save(model_1, f\"./models_1/model_1_{cur}.pth\")\n",
    "# Сохраняем информацию о гиперпараметрах модели\n",
    "with open(\"./models_1/info.txt\", 'a') as f:\n",
    "    print(f\"{cur}. n_hidden = {n_hidden_1}, n_hidden_layers = {n_hidden_layers_1}, lr = {lr_1}, n_epochs = {n_epochs_1}, batch_size = {batch_size_1}, sigmoid_rate = {sigmoid_rate_1}, MSE: {round(mean_squared_error(y_val_1, model_1.predict(X_val_1)))}\", file=f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
