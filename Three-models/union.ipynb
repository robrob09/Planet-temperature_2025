{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код, который объединяет решения `classification`, `model_0`, `model_1`\n",
    "Для тестовых данных выполняем следующие шаги:\n",
    "1. Решаем задачу классификации (определяем, будет ли точка близка к медианному значению или нет)\n",
    "2. Для точек, близких к медианном значению, для предсказания используем `model_0`\n",
    "2. Для точек, не близких к медианном значению, для предсказания используем `model_1`\n",
    "\n",
    "Оказывается, что такой комбинированный подход работает лучше, чем просто одна нейронная сеть, обучення на всех данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код из classification.ipynb\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.X[i]).float(), torch.tensor(self.y[i]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код из model_0.ipynb и model_1.ipynb\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.X[i]).float(), torch.tensor(self.y[i]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код из classification.ipynb\n",
    "class NNClassification:\n",
    "    def __init__(self, n_hidden=128, n_hidden_layers=2, lr=1e-3, n_epochs=200, batch_size=32, sigmoid_rate=0):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.sigmoid_rate = sigmoid_rate\n",
    "        self.model = None\n",
    "        self.train_loss = None\n",
    "        self.val_loss = None\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        n_val_X = X_train.shape[-1]\n",
    "        n_val_y = len(np.unique(y_train))\n",
    "\n",
    "        lawyers = [nn.Linear(n_val_X, self.n_hidden), nn.ReLU()]\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            lawyers.append(nn.Linear(self.n_hidden, self.n_hidden))\n",
    "            if random.random() < self.sigmoid_rate:\n",
    "                lawyers.append(nn.Sigmoid())\n",
    "            else:\n",
    "                lawyers.append(nn.ReLU())\n",
    "        lawyers.append(nn.Linear(self.n_hidden, n_val_y))\n",
    "        self.model = nn.Sequential(*lawyers)\n",
    "        \n",
    "        optim = Adam(self.model.parameters(), lr=self.lr)\n",
    "        dataset = ClassificationDataset(X_train, y_train)\n",
    "        dataloader = DataLoader(dataset, shuffle=True, batch_size=self.batch_size)\n",
    "        self.train_loss = []\n",
    "\n",
    "        val_dataset = ClassificationDataset(X_val, y_val)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "        self.val_loss = []\n",
    "\n",
    "        best_model = None\n",
    "        best_num_epoch = None\n",
    "        best_val_loss = None\n",
    "        \n",
    "        for epoch in range(1, self.n_epochs + 1):\n",
    "            losses = []\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                y_pred = self.model(x_batch)\n",
    "                loss = F.cross_entropy(y_pred, y_batch)\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                losses.append(loss.detach().item())\n",
    "            self.train_loss.append(np.mean(losses))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                losses = []\n",
    "                for x_batch, y_batch in val_dataloader:\n",
    "                    y_pred = self.model(x_batch)\n",
    "                    loss = F.cross_entropy(y_pred, y_batch)\n",
    "                    losses.append(loss.detach().item())\n",
    "                self.val_loss.append(np.mean(losses))\n",
    "            \n",
    "            if best_val_loss == None or best_val_loss > self.val_loss[-1]:\n",
    "                best_val_loss = self.val_loss[-1]\n",
    "                best_num_epoch = epoch\n",
    "                best_model = deepcopy(self.model)\n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"Epoch {epoch}, loss {self.val_loss[-1]}\")\n",
    "        self.model = deepcopy(best_model)\n",
    "        print(f\"Best loss was at {best_num_epoch} epoch: {best_val_loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor(X).float()\n",
    "            y = self.model(X).numpy()\n",
    "            return np.argmax(y, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код из model_0.ipynb и model_1.ipynb\n",
    "class NNRegression:\n",
    "    def __init__(self, n_hidden=128, n_hidden_layers=2, lr=1e-3, n_epochs=200, batch_size=32, sigmoid_rate=0):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.sigmoid_rate = sigmoid_rate\n",
    "        self.model = None\n",
    "        self.train_loss = None\n",
    "        self.val_loss = None\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        n_val_X = X_train.shape[-1]\n",
    "        n_val_y = y_train.shape[-1]\n",
    "\n",
    "        lawyers = [nn.Linear(n_val_X, self.n_hidden), nn.ReLU()]\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            lawyers.append(nn.Linear(self.n_hidden, self.n_hidden))\n",
    "            if random.random() < self.sigmoid_rate:\n",
    "                lawyers.append(nn.Sigmoid())\n",
    "            else:\n",
    "                lawyers.append(nn.ReLU())\n",
    "        lawyers.append(nn.Linear(self.n_hidden, n_val_y))\n",
    "        self.model = nn.Sequential(*lawyers)\n",
    "        \n",
    "        optim = Adam(self.model.parameters(), lr=self.lr)\n",
    "        dataset = RegressionDataset(X_train, y_train)\n",
    "        dataloader = DataLoader(dataset, shuffle=True, batch_size=self.batch_size)\n",
    "        self.train_loss = []\n",
    "\n",
    "        val_dataset = RegressionDataset(X_val, y_val)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "        self.val_loss = []\n",
    "\n",
    "        best_model = None\n",
    "        best_num_epoch = None\n",
    "        best_val_loss = None\n",
    "        \n",
    "        for epoch in range(1, self.n_epochs + 1):\n",
    "            losses = []\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                y_pred = self.model(x_batch)\n",
    "                loss = F.mse_loss(y_pred, y_batch)\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                losses.append(loss.detach().item())\n",
    "            self.train_loss.append(np.mean(losses))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                losses = []\n",
    "                for x_batch, y_batch in val_dataloader:\n",
    "                    y_pred = self.model(x_batch)\n",
    "                    loss = F.mse_loss(y_pred, y_batch)\n",
    "                    losses.append(loss.detach().item())\n",
    "                self.val_loss.append(np.mean(losses))\n",
    "            \n",
    "            if best_val_loss == None or best_val_loss > self.val_loss[-1]:\n",
    "                best_val_loss = self.val_loss[-1]\n",
    "                best_num_epoch = epoch\n",
    "                best_model = deepcopy(self.model)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, loss {round(self.val_loss[-1])}\")\n",
    "        self.model = deepcopy(best_model)\n",
    "        print(f\"Best loss was at {best_num_epoch} epoch: {round(best_val_loss)}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor(X).float()\n",
    "            y = self.model(X).numpy()\n",
    "            return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classification_1.pth': 1, 'classification_4.pth': 4, 'classification_8.pth': 8, 'classification_12.pth': 12, 'classification_17.pth': 17, 'classification_18.pth': 18}\n",
      "{'model_0_2.pth': 2}\n",
      "{'model_1_2.pth': 2, 'model_1_5.pth': 5}\n"
     ]
    }
   ],
   "source": [
    "classifications = {}\n",
    "with open('./classifications/info.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        id = int(line[:line.find('.')])\n",
    "        score = float(line[line.rfind(' ') + 1:])\n",
    "        # Используем только те модели классификаци,\n",
    "        # которые дают accurace больше 91 %\n",
    "        if score > 0.91:\n",
    "            classifications[f'classification_{id}.pth'] = id\n",
    "\n",
    "models_0 = {}\n",
    "with open('./models_0/info.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        id = int(line[:line.find('.')])\n",
    "        score = int(line[line.rfind(' ') + 1:])\n",
    "        # Используем только те модели регрессии,\n",
    "        # которые дают MSE меньше 100\n",
    "        if score < 100:\n",
    "            models_0[f'model_0_{id}.pth'] = id\n",
    "\n",
    "models_1 = {}\n",
    "with open('./models_1/info.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        id = int(line[:line.find('.')])\n",
    "        score = int(line[line.rfind(' ') + 1:])\n",
    "        # Используем только те модели регрессии,\n",
    "        # которые дают MSE меньше 5000\n",
    "        if score < 5000:\n",
    "            models_1[f'model_1_{id}.pth'] = id\n",
    "\n",
    "# Выводим модели, которые будут использованы для генерации решений\n",
    "print(classifications)\n",
    "print(models_0)\n",
    "print(models_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_test = pd.read_csv(\"./data/x_test.csv\", header=None)\n",
    "np_X_test = df_X_test.to_numpy()\n",
    "n = np_X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./submissions/submission_1_2_2.csv\n",
      "./submissions/submission_1_2_5.csv\n",
      "./submissions/submission_4_2_2.csv\n",
      "./submissions/submission_4_2_5.csv\n",
      "./submissions/submission_8_2_2.csv\n",
      "./submissions/submission_8_2_5.csv\n",
      "./submissions/submission_12_2_2.csv\n",
      "./submissions/submission_12_2_5.csv\n",
      "./submissions/submission_17_2_2.csv\n",
      "./submissions/submission_17_2_5.csv\n",
      "./submissions/submission_18_2_2.csv\n",
      "./submissions/submission_18_2_5.csv\n"
     ]
    }
   ],
   "source": [
    "for classification_name in classifications:\n",
    "    model_classification = torch.load(os.path.join('./classifications/', classification_name), weights_only=False)\n",
    "    # 1. Классификация\n",
    "    y_bin = model_classification.predict(np_X_test)\n",
    "\n",
    "    X_test_0 = []\n",
    "    X_test_1 = []\n",
    "    D_0 = {}\n",
    "    D_1 = {}\n",
    "    for i in range(n):\n",
    "        if y_bin[i] == 0:\n",
    "            D_0[i] = len(X_test_0)\n",
    "            X_test_0.append(np_X_test[i])\n",
    "        else:\n",
    "            D_1[i] = len(X_test_1)\n",
    "            X_test_1.append(np_X_test[i])\n",
    "\n",
    "    X_test_0 = np.array(X_test_0)\n",
    "    X_test_1 = np.array(X_test_1)\n",
    "\n",
    "    for model_0_name in models_0:\n",
    "        # 2. Регрессия для точек из группы 0\n",
    "        model_0 = torch.load(os.path.join('./models_0/', model_0_name), weights_only=False)\n",
    "        y_pred_0 = model_0.predict(X_test_0)\n",
    "        for model_1_name in models_1:\n",
    "            model_1 = torch.load(os.path.join('./models_1/', model_1_name), weights_only=False)\n",
    "            # 3. Регрессия для точек из группы 1\n",
    "            y_pred_1 = model_1.predict(X_test_1)\n",
    "\n",
    "            # Объединяем предсказания для точек из обеих групп\n",
    "            y_pred = [y_pred_0[D_0[i]] if y_bin[i] == 0 else y_pred_1[D_1[i]] for i in range(n)]\n",
    "            y_pred = np.array(y_pred)[:, 0]\n",
    "            df_y_pred = pd.DataFrame(y_pred)\n",
    "            # Сохраняем файл с предсказаниями\n",
    "            name = f'./submissions/submission_{classifications[classification_name]}_{models_0[model_0_name]}_{models_1[model_1_name]}.csv'\n",
    "            # Выводим название файла\n",
    "            print(name)\n",
    "            df_y_pred.to_csv(name, header=False, index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сгенерируем \"усреднённое\" решение\n",
    "# Используем только модели из следующих списков\n",
    "clf_list = [1, 4]\n",
    "m0_list = [2]\n",
    "m1_list = [2, 5]\n",
    "num = 0\n",
    "a = np.array([0. for i in range(n)])\n",
    "for clf in clf_list:\n",
    "    for m0 in m0_list:\n",
    "        for m1 in m1_list:\n",
    "            a += pd.read_csv(f\"./submissions/submission_{clf}_{m0}_{m1}.csv\", header=None).to_numpy()[:, 0]\n",
    "            num += 1\n",
    "\n",
    "a /= num\n",
    "# a - массив, содержащий усреднённые предсказания\n",
    "# print(a)\n",
    "pd.DataFrame(a).to_csv(\"./submissions/submission_avg.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение `./submissions/submission_avg.csv` демонстрирует достаточно высокую точность предсказаний и оказывается лучше других."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Код который конвертирует все .csv файлы с решением, лежащие в папке 'submissions',\n",
    "# в требуемый .zip формат в папке 'send'\n",
    "\n",
    "for filename in os.listdir('./submissions'):\n",
    "    if filename.startswith('submission') and filename.endswith('.csv'):\n",
    "        name = filename.split('.')[0]\n",
    "        path = os.path.join('./send', name)\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "        os.mkdir(f'./send/{name}')\n",
    "        shutil.copy(os.path.join('./submissions', filename), os.path.join(path, filename))\n",
    "        os.rename(os.path.join('./send', name, filename), os.path.join(path, '01.out'))\n",
    "        with ZipFile(os.path.join(path, 'output.zip'), 'w') as mz:\n",
    "            mz.write(os.path.join(path, '01.out'), '01.out')\n",
    "        os.remove(os.path.join(path, '01.out'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
